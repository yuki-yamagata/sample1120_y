{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7f0f15",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒç¢ºèªã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "# PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"mps\":\n",
    "    print(\"\\nâš ï¸ MPS support is preliminary. SAM 2 might give numerically different outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903ab64",
   "metadata": {},
   "source": [
    "## 2. è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "\n",
    "ã“ã“ã§è§£æã—ãŸã„ãƒ“ãƒ‡ã‚ªã‚„å‡ºåŠ›å…ˆã‚’æŒ‡å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "ROOT = Path.cwd()\n",
    "\n",
    "# å…¥åŠ›ãƒ“ãƒ‡ã‚ªè¨­å®š\n",
    "VIDEO_MP4 = ROOT / \"videos\" / \"D1AT_20251029_Trim_night_30sec.mp4\"\n",
    "FRAMES_DIR = ROOT / \"videos\" / \"night_30sec\"\n",
    "\n",
    "# SAM2 ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "CONFIG_NAME = \"configs/sam2.1/sam2.1_hiera_l.yaml\"  # Hydraãƒ‘ã‚¹\n",
    "CHECKPOINT = ROOT / \"sam2\" / \"checkpoints\" / \"sam2.1_hiera_large.pt\"\n",
    "\n",
    "# å‡ºåŠ›è¨­å®š\n",
    "OUTPUT_CSV = ROOT / \"positions\" / \"mouse_tracks_night.csv\"\n",
    "SEGMENTED_DIR = FRAMES_DIR.parent / f\"{FRAMES_DIR.name}_segmented\"\n",
    "\n",
    "# ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°è¨­å®š\n",
    "NUM_OBJECTS = 2  # ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ãƒã‚¦ã‚¹ã®å€‹ä½“æ•°\n",
    "ANN_FRAME_IDX = 0  # ã‚¯ãƒªãƒƒã‚¯æŒ‡å®šã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·\n",
    "\n",
    "print(f\"Video: {VIDEO_MP4}\")\n",
    "print(f\"Frames dir: {FRAMES_DIR}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT}\")\n",
    "print(f\"Output CSV: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd311f",
   "metadata": {},
   "source": [
    "## 3. ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰\n",
    "\n",
    "ãƒ“ãƒ‡ã‚ªã‹ã‚‰JPEGãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã¾ã™ã€‚æ—¢ã«ãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08172738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_frames(frames_dir: Path, video_path: Path) -> None:\n",
    "    \"\"\"ãƒ“ãƒ‡ã‚ªã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã‚’æŠ½å‡ºï¼ˆæ—¢ã«ã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\"\"\"\n",
    "    if frames_dir.exists():\n",
    "        if any(frames_dir.glob(\"*.jpg\")) or any(frames_dir.glob(\"*.jpeg\")):\n",
    "            print(f\"Frames already exist in {frames_dir}\")\n",
    "            return\n",
    "    \n",
    "    if not video_path.exists():\n",
    "        raise FileNotFoundError(f\"Video not found: {video_path}\")\n",
    "    \n",
    "    frames_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    index = 0\n",
    "    print(f\"Extracting frames from {video_path} to {frames_dir} ...\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_path = frames_dir / f\"{index:05d}.jpg\"\n",
    "        cv2.imwrite(str(frame_path), frame)\n",
    "        index += 1\n",
    "    cap.release()\n",
    "    \n",
    "    if index == 0:\n",
    "        raise RuntimeError(f\"No frames extracted from {video_path}\")\n",
    "    print(f\"âœ… Extracted {index} frames.\")\n",
    "\n",
    "# ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Ÿè¡Œ\n",
    "ensure_frames(FRAMES_DIR, VIDEO_MP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501d400",
   "metadata": {},
   "source": [
    "## 4. ãƒ•ãƒ¬ãƒ¼ãƒ ä¸€è¦§ã®å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _frame_sort_key(path: Path):\n",
    "    stem = path.stem\n",
    "    return int(stem) if stem.isdigit() else stem\n",
    "\n",
    "frame_paths = sorted(\n",
    "    [p for p in FRAMES_DIR.iterdir() if p.suffix.lower() in {\".jpg\", \".jpeg\"}],\n",
    "    key=_frame_sort_key,\n",
    ")\n",
    "\n",
    "if not frame_paths:\n",
    "    raise RuntimeError(f\"No JPEG frames found in {FRAMES_DIR}\")\n",
    "\n",
    "print(f\"âœ… Found {len(frame_paths)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a42ba3",
   "metadata": {},
   "source": [
    "## 5. SAM2ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "if not CHECKPOINT.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT}\")\n",
    "\n",
    "predictor = build_sam2_video_predictor(CONFIG_NAME, str(CHECKPOINT), device=device)\n",
    "print(\"âœ… SAM2 model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea05bb",
   "metadata": {},
   "source": [
    "## 6. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', \n",
    "               s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', \n",
    "               s=marker_size, edgecolor='white', linewidth=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b07103",
   "metadata": {},
   "source": [
    "## 7. æ¨è«–çŠ¶æ…‹ã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=str(FRAMES_DIR))\n",
    "print(\"âœ… Inference state initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04a2d1",
   "metadata": {},
   "source": [
    "## 8. å¯¾è±¡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¯ãƒªãƒƒã‚¯æŒ‡å®š\n",
    "\n",
    "**é‡è¦:** OpenCVã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒé–‹ãã®ã§ã€å„ãƒã‚¦ã‚¹ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‹ã‚‰Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer:\n",
    "    def __init__(self, img_path: Path):\n",
    "        self.img_path = img_path\n",
    "        self.img = cv2.imread(str(img_path))\n",
    "        if self.img is None:\n",
    "            raise FileNotFoundError(f\"Failed to load image: {img_path}\")\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self._clicked = False\n",
    "\n",
    "    def onMouse(self, event, x, y, flags, params):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            print(f\"Clicked at ({x}, {y})\")\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            self._clicked = True\n",
    "            self.img_marked = self.img.copy()\n",
    "            cv2.circle(self.img_marked, (self.x, self.y), 8, (0, 0, 255), -1)\n",
    "            cv2.imshow(str(self.img_path), self.img_marked)\n",
    "\n",
    "    def point_gui(self):\n",
    "        cv2.imshow(str(self.img_path), self.img)\n",
    "        cv2.setMouseCallback(str(self.img_path), self.onMouse)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        if not self._clicked:\n",
    "            raise RuntimeError(\"No click detected. Please click before closing.\")\n",
    "\n",
    "prompts = {}\n",
    "\n",
    "for obj_idx in range(NUM_OBJECTS):\n",
    "    ann_obj_id = obj_idx + 1\n",
    "    print(f\"\\nğŸ“ Please click on object {ann_obj_id}/{NUM_OBJECTS} in the window, then press Enter\")\n",
    "    \n",
    "    pointer = Pointer(frame_paths[ANN_FRAME_IDX])\n",
    "    pointer.point_gui()\n",
    "    \n",
    "    points = np.array([[pointer.x, pointer.y]], dtype=np.float32)\n",
    "    labels = np.array([1], np.int32)\n",
    "    prompts[ann_obj_id] = points, labels\n",
    "    \n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ANN_FRAME_IDX,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "    \n",
    "    # ç¢ºèªç”¨ã®å¯è¦–åŒ–\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title(f\"Object {ann_obj_id} - Frame {ANN_FRAME_IDX}\")\n",
    "    plt.imshow(Image.open(frame_paths[ANN_FRAME_IDX]))\n",
    "    for i, out_obj_id in enumerate(out_obj_ids):\n",
    "        show_points(*prompts[out_obj_id], plt.gca())\n",
    "        show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Clicked {NUM_OBJECTS} objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af320b0",
   "metadata": {},
   "source": [
    "## 9. å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã¸ã®ä¼æ’­ã¨ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "\n",
    "å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚ãŸã‚Šã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã€çµæœç”»åƒã‚’ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "SEGMENTED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã¸ä¼æ’­\n",
    "video_segments = {}\n",
    "print(\"\\nğŸ”„ Propagating masks through video...\")\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "print(f\"âœ… Propagation complete for {len(video_segments)} frames\")\n",
    "\n",
    "# ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”»åƒã®å‡ºåŠ›\n",
    "print(\"\\nğŸ’¾ Saving segmented images...\")\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in tqdm(range(len(frame_paths)), desc=\"Saving images\"):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"Frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(frame_paths[out_frame_idx]))\n",
    "    \n",
    "    if out_frame_idx in video_segments:\n",
    "        for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "            show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SEGMENTED_DIR / f\"s_{frame_paths[out_frame_idx].name}\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(f\"âœ… Saved segmented images to {SEGMENTED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbda73b",
   "metadata": {},
   "source": [
    "## 10. é‡å¿ƒåº§æ¨™ã®è¨ˆç®—ã¨ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = defaultdict(list)\n",
    "\n",
    "for frame_idx, obj_dict in video_segments.items():\n",
    "    for obj_id, mask in obj_dict.items():\n",
    "        m = mask.squeeze()\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        ys, xs = np.nonzero(m)\n",
    "        cx = xs.mean()\n",
    "        cy = ys.mean()\n",
    "        tracks[obj_id].append((frame_idx, float(cx), float(cy)))\n",
    "\n",
    "# CSVä¿å­˜\n",
    "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "with OUTPUT_CSV.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"object_id\", \"frame_idx\", \"cx\", \"cy\"])\n",
    "    for obj_id, traj in tracks.items():\n",
    "        for frame_idx, cx, cy in traj:\n",
    "            writer.writerow([obj_id, frame_idx, cx, cy])\n",
    "\n",
    "print(f\"âœ… Saved centroid CSV to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85740d7b",
   "metadata": {},
   "source": [
    "## 11. è»Œè·¡ã®å¯è¦–åŒ–\n",
    "\n",
    "ä¿å­˜ã—ãŸCSVã‹ã‚‰è»Œè·¡ã‚’èª­ã¿è¾¼ã‚“ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd76613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVã®èª­ã¿è¾¼ã¿\n",
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "# ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’æ™‚é–“ã«å¤‰æ›\n",
    "FPS = 25.0\n",
    "df[\"time\"] = df[\"frame_idx\"] / FPS\n",
    "\n",
    "print(f\"Loaded {len(df)} data points for {df['object_id'].nunique()} objects\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fb317",
   "metadata": {},
   "source": [
    "### 11.1 æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆï¼ˆxåº§æ¨™ã¨yåº§æ¨™ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\"orange\", \"green\", \"blue\", \"purple\", \"red\", \"brown\", \"magenta\", \"cyan\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, sharex=True, figsize=(12, 6))\n",
    "\n",
    "for idx, (obj_id, g) in enumerate(df.groupby(\"object_id\")):\n",
    "    color = palette[idx % len(palette)]\n",
    "    axes[0].plot(g[\"time\"], g[\"cx\"], label=f\"Mouse {obj_id}\", color=color)\n",
    "    axes[1].plot(g[\"time\"], g[\"cy\"], label=f\"Mouse {obj_id}\", color=color)\n",
    "\n",
    "axes[0].set_ylabel(\"x (pixels)\")\n",
    "axes[1].set_ylabel(\"y (pixels)\")\n",
    "axes[1].set_xlabel(\"Time (s)\")\n",
    "axes[0].set_title(\"X coordinate vs Time\")\n",
    "axes[1].set_title(\"Y coordinate vs Time\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca6253",
   "metadata": {},
   "source": [
    "### 11.2 2Dè»Œè·¡ãƒ—ãƒ­ãƒƒãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c166c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for idx, (obj_id, g) in enumerate(df.groupby(\"object_id\")):\n",
    "    color = palette[idx % len(palette)]\n",
    "    plt.plot(g[\"cx\"], g[\"cy\"], marker=\".\", linestyle=\"-\", \n",
    "             label=f\"Mouse {obj_id}\", color=color, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"x (pixels)\")\n",
    "plt.ylabel(\"y (pixels)\")\n",
    "plt.title(\"2D Trajectories\")\n",
    "plt.gca().invert_yaxis()  # ç”»åƒåº§æ¨™ç³»ã«åˆã‚ã›ã‚‹\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88c6a8",
   "metadata": {},
   "source": [
    "## å®Œäº†\n",
    "\n",
    "ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼\n",
    "\n",
    "### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "- ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”»åƒ: `videos/night_30sec_segmented/`\n",
    "- é‡å¿ƒåº§æ¨™CSV: `positions/mouse_tracks_night.csv`\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "- CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’Excelã‚„ä»–ã®ãƒ„ãƒ¼ãƒ«ã§è§£æ\n",
    "- ã‚ˆã‚Šè©³ç´°ãªçµ±è¨ˆè§£æï¼ˆç§»å‹•è·é›¢ã€é€Ÿåº¦ãªã©ï¼‰\n",
    "- ç•°ãªã‚‹ãƒ“ãƒ‡ã‚ªã§ã®è§£æ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
